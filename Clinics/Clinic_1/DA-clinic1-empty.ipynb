{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3a9aff",
   "metadata": {},
   "source": [
    "(Group 24)\n",
    "\n",
    "(Benjamin Fletcher, Konrad Retzlaff, Sebastian Schm√ºlling)\n",
    "\n",
    "(i6308020, i6304238 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ab070",
   "metadata": {},
   "source": [
    "**Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow)**: *list websites where you found code (or other info) as well as include information on how you used genAI tools*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a108445",
   "metadata": {},
   "source": [
    "# Data Analysis, Clinic 1\n",
    "# FIETS: Fundamentele Innovatie En Technologie in Scholing\n",
    "## Met FIETS blijft het onderwijs vooruitgaan, zelfs tegen de wind in!\n",
    "\n",
    "---\n",
    "\n",
    "By completing and delivering the clinic tasks you will know how to :\n",
    "\n",
    "- Load data and handle data using pandas;\n",
    "- Navigate the documentation of Python packages by yourself;\n",
    "- Filter and tidy up **noisy** real-world datasets;\n",
    "- Aggregate your data in different (and hopefully helpful) ways;\n",
    "- Use EDA to learn more about your data\n",
    "- Create and interpret informative visualizations to explore the data set\n",
    "- Derive meaningful insights for the societal impact of datasets\n",
    "\n",
    "---\n",
    "**Important Dates.**\n",
    "\n",
    "- Clinic 1 release: Thu 30 Jan 2024\n",
    "- Clinic 1 due: Fri 07 Feb 2024 late night, wildcards available\n",
    "\n",
    "**Instructions for the deliverable:**\n",
    "\n",
    "* You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you may do so, but must justify your choice.\n",
    "\n",
    "* Make sure that you include a proper amount/mix of comments, results and code. More specifically, be sure to provide a concise textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice. To avoid confusion: use short comments for longer code answers.\n",
    "\n",
    "* For questions containing the /Discuss:/ prefix, answer not with code, but with a textual explanation (in markdown).\n",
    "\n",
    "* Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "\n",
    "* Please write all your comments in English, and use meaningful variable names (as possible) in your code. \n",
    "\n",
    "* In the end, make sure that all cells are executed properly and everything you need to show is in your (execucted) notebook. We will not run your notebook for you! \n",
    "\n",
    "- In continuation to the previous point, interactive plots, such as those generated using the ‚Äòplotly‚Äô package, should be strictly avoided! Make sure to print results and/or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "* You are asked to deliver **only your executed notebook file, .ipnyb** and nothing else. If you deliver other files, we will not grade anything.\n",
    "\n",
    "* Honor code applies to these tasks. If you are not certain about an action, consult with Jerry.\n",
    "\n",
    "**A Note from Jerry on using Language Models (LMs)**\n",
    "\n",
    "If you try hard enough, you will likely get away with cheating (that does not only apply to LMs). Fortunately, my job is not to police, but rather to educate you. So, please consider the following:\n",
    "\n",
    "I assume that you are taking this course to learn something! LMs are not always right ([they often fail in silly ways](https://community.openai.com/t/why-9-11-is-larger-than-9-9-incredible/869824/4)). This course should prepare you to detect when they are wrong!\n",
    "\n",
    "I don't restrict the use of LMs because I see the value of being helped when coding (esp. in the context of pandas dataframes nightmare :)). Based on what we saw last year in your notebooks, it's pretty clear when you \"copy\" some code and then you struggle to interpret the results. This is the essence of this course and of the skills you should try build for yourself: Many people can run fancy models these days but not many people can interpret the results correctly. Try to be the latter ones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b54879",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "AI is booming! Newspapers, influencers and your relatives all agree that AI is important. But while almost everyone agrees that AI is the future, much is unclear about what that future esp. in critical sectors like education looks like...\n",
    "\n",
    "Freshly graduated from a top Dutch university in Limburg, you are hired by the Dutch government to advise on a large-scale ‚Äúeducation innovation‚Äù initiative code-named \"FIETS\" (Flexibele Innovatie voor Effici√´nte Toepassing in Scholing). With higher education facing severe budget cuts, the government is looking for creative solutions to \"do more with less.\" Convinced by the stunning progress in language modeling, officials believe LLMs could help battle growing teacher shortages and reduce costs by automating parts of the education process. Your job description: investigate which LMs might be best suited to plug the gaps without draining the budget!\n",
    "\n",
    "You are handed the results of three LMs on the [‚ÄúMassive Multitask Language Understanding (MMLU)‚Äù](https://arxiv.org/abs/2009.03300) dataset  to compare. This famous dataset consists of 57 subjects with multiple-choice questions, covering diverse subjects like mathematics, computer science, history, and law. Most providers of state-of-the-art LMs use this dataset to showcase the versatility of their latest models. Unfortunately, the intern responsible for collecting the results, didn‚Äôt pay attention during DACS KEN3450: Data Analysis. As a result, the collected datasets are slightly corrupted. Jammer!\n",
    "\n",
    "The success of FIETS depends on your ability to make sense of the messy data and recommend the best model to keep the Dutch education system pedaling forward‚Äîdespite uphill challenges like funding shortages and a skeptical academic community!\n",
    "\n",
    "### A very brief primer on Language Models\n",
    "We studied LLMs in the context of the NLP course but here is a short reminder. Language models (LMs) are sophisticated statistical models designed to understand and generate human-like text. At their core, LMs are trained to predict the most likely continuation of a given input text. For example, given the input \"The cat sat on the,\" an LM might predict \"mat\" as a likely continuation.\n",
    "LMs are trained on vast text samples from various sources, including books, websites, and social media. This extensive training allows them to capture patterns and relationships in language, enabling them to generate coherent and contextually appropriate text across a wide range of topics and styles.\n",
    "\n",
    "While LMs can produce text that appears to be written by intelligent humans, it's important to note that their capabilities can diverge from human intelligence in unexpected ways. They may sometimes generate factually incorrect information or struggle with complex reasoning tasks.\n",
    "\n",
    "Two key concepts in understanding LMs are:\n",
    "1. **Tokens**: LMs process text using \"tokens\" rather than individual characters. Tokens can be words, parts of words, or punctuation marks. For example, the sentence \"I love AI!\" might be tokenized as [\"I\", \"love\", \"AI\", \"!\"]. Tokenization is the first step in both training and using an LM.\n",
    "2. **Context**: The input text provided to an LM is called the \"context.\" This context informs the model's predictions or generations. A longer or more specific context often leads to more accurate and relevant outputs.\n",
    "\n",
    "[See: Wikipedia entry on language models](https://en.wikipedia.org/wiki/Large_language_model)\n",
    "\n",
    "###  Files for this assignment\n",
    "This assignment is divided into three tasks, each of which should bring you a step closer to providing a recommendation toward project the objectives of FIETS:\n",
    "\n",
    "- **Task 1**: Inspecting the results and getting your first model ranking\n",
    "- **Task 2**: Inspecting the underlying data used to generate the results for possible biases\n",
    "- **Task 3**: Learning about tokens and providing a final recommendation\n",
    "\n",
    "\n",
    "```\n",
    "üìÅ FIETS\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìÑ clinic1.ipynb (the file you're currently reading!)\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ üìÅ data\n",
    "    ‚îú‚îÄ‚îÄ üìÅ task_1\n",
    "    ‚îú‚îÄ‚îÄ üìÅ task_2\n",
    "    ‚îî‚îÄ‚îÄ üìÅ task_2.5\n",
    "```   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8fd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a7be4",
   "metadata": {},
   "source": [
    "## Task 1 (18 points): What's in an average anyway?\n",
    "\n",
    "The files needed to complete task 1 can be found in the folder \"`data/task_1/`:\n",
    "```\n",
    "task_1/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ mmlu_data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores/\n",
    "    ‚îú‚îÄ‚îÄ lm_X.csv\n",
    "    ‚îú‚îÄ‚îÄ lm_Y.csv\n",
    "    ‚îî‚îÄ‚îÄ lm_Z.csv\n",
    "```\n",
    "\n",
    "We will start by loading, (manually) inspecting, and cleaning the data. Although it doesn't seem \"glamorous\" (nor is it particularly fun...) - manually inspecting data is extremely important! In fact, it's one of the few things most AI and Data Science researchers agree on :). Next, we will take a first pass on ordering our Olympic podium between three LMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16518de6",
   "metadata": {},
   "source": [
    "### 1.1 (1 pt)\n",
    " \n",
    "Load the subfiles contained in the `mmlu_data` and `lm_scores` folders into separate dataframes:\n",
    "- `df_test`\n",
    "- `df_x`\n",
    "- `df_y`\n",
    "- `df_z`\n",
    "\n",
    "for each, print their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80eabdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test:  (14042, 8)\n",
      "df_x:  (13882, 2)\n",
      "df_y:  (13978, 2)\n",
      "df_z:  (13923, 2)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/task_1/mmlu_data/test.csv')\n",
    "df_x = pd.read_csv(\"data/task_1/lm_scores/lm_X.csv\")\n",
    "df_y = pd.read_csv(\"data/task_1/lm_scores/lm_Y.csv\")\n",
    "df_z = pd.read_csv(\"data/task_1/lm_scores/lm_Z.csv\")\n",
    "\n",
    "print('df_test: ', df_test.shape)\n",
    "print('df_x: ', df_x.shape)\n",
    "print('df_y: ', df_y.shape)\n",
    "print('df_z: ', df_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c5e2",
   "metadata": {},
   "source": [
    "### 1.2 (4 pt)\n",
    "Unfortunately, LMs don't always output the format we want. In the column `result`, the value should be one of A, B, C, or D. \n",
    "\n",
    "A. For each of the LM score dataframes, use a `value_counts()` operation and print the results. \n",
    "\n",
    "B. /Discuss:/ Inspect the results and describe the types of answer formats you see. Besides the \"expected\" case, you should be able to find at least four unexpected formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeebdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id  result   \n",
      "0            B            1\n",
      "9418         B            1\n",
      "9407         B            1\n",
      "9408         C            1\n",
      "9409         Answer: B    1\n",
      "                         ..\n",
      "4643         B            1\n",
      "4644         Answer: B    1\n",
      "4645         C            1\n",
      "4646         A            1\n",
      "14041        Answer: A    1\n",
      "Name: count, Length: 13882, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "#Set X\n",
    "print(df_x.value_counts(dropna=False))\n",
    "print(type(df_x.value_counts()))\n",
    "#Set Y\n",
    "#print(df_y.value_counts())\n",
    "#set Z\n",
    "#print(df_z.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709b59db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "A                                                                                                                 1657\n",
      "Answer: A                                                                                                         1398\n",
      "B                                                                                                                  793\n",
      "Answer: B                                                                                                          760\n",
      "C                                                                                                                  622\n",
      "                                                                                                                  ... \n",
      "judicial activism, so the answer is A                                                                                1\n",
      "creating insurmountable obstacles to the founding of factions, so the answer is A                                    1\n",
      "A congressperson who retires to take a position teaching political science at a university, so the answer is A       1\n",
      "David Hume, so the answer is D                                                                                       1\n",
      "Brahminic orthodoxy, so the answer is A                                                                              1\n",
      "Name: count, Length: 141, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# B\n",
    "\n",
    "\n",
    "#We can see from the initial outputs that some of the results are simple A,B,C,D as expected, however some of the formats have extra text, \n",
    "# either specifying the answer or explaining it. \n",
    "#Unexpected Type Examples\n",
    "#'Answer: D'\n",
    "#'The demand for labor is derived from the demand for the products produced by labor., so the answer is D'\n",
    "#I wrote this code which groups results by the expected formats. Here we can see we have the normal types, the types with 'Answer:' and also multiple explanations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#On Sebastians note: \n",
    "\n",
    "#Also oberserve that value_counts() retunds a Series object,with unique values as index and their counts as values. If you compare the lenght of the Series object with the number of\n",
    "#rows in the dataframe, you can see that the number of unique values is less than the number of rows in the dataframe. This is because the series object does not count NaN values and also\n",
    "#does only count unique values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Write proper markdown file\n",
    "\n",
    "\n",
    "unexpected = df_x.loc[~df_x[\"result\"].isin([\"A\", \"B\", \"C\", \"D\"]), \"result\"]\n",
    "print(unexpected.value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98377d82",
   "metadata": {},
   "source": [
    "### 1.3 (5 pt)\n",
    "Oh oh... That doesn't look great. Simply dropping all invalid answers seems overly wasteful, yet fixing all of these looks like a mess! Instead, let's focus for now on fixing just those answers of length < 10 characters that require only a single `str.replace()` operation. \n",
    "\n",
    "For example, if the answer looks like `--A--`, we could fix this by using the following simple function:\n",
    "\n",
    "```\n",
    "def clean_answer(s, pattern='-'):\n",
    "    return str(s).replace(pattern, '')\n",
    "\n",
    "dirty_answer = '--A--'\n",
    "clean_answer = clean_answer(dirty_answer)\n",
    "```\n",
    "\n",
    "A. Filter the three score dataframes to include only answers with less than 10 characters. Make a deep copy of the dataframes as you filter them.\n",
    "\n",
    "B. Modify the `clean_answer()` example function to clean the answers in the filtered data frames using the `apply()` functionality. Finally, make sure **all remaining answers are one of `A, B, C, or D`.**\n",
    "\n",
    "C. /Discuss:/ Compare the sizes of the original and filtered data frames. What do you see? Why might this be a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id  result   \n",
      "0            B            1\n",
      "9428         Answer: B    1\n",
      "9403         Answer: D    1\n",
      "9404         B            1\n",
      "9405         D            1\n",
      "                         ..\n",
      "4644         Answer: B    1\n",
      "4645         C            1\n",
      "4646         A            1\n",
      "4647         A            1\n",
      "14041        Answer: A    1\n",
      "Length: 13509, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "df_x_filtered = df_x[df_x[\"result\"].str.len() < 10].copy(deep=True)\n",
    "df_y_filtered = df_y[df_y[\"result\"].str.len() < 10].copy(deep=True)\n",
    "df_z_filtered = df_z[df_z[\"result\"].str.len() < 10].copy(deep=True)\n",
    "#Based on the documentation, this should sort it all out so we only keep stuff less than 10\n",
    "##From above, we could notice that any of the long explanation always end with the actual short result\n",
    "#Eg, David Hume, so the answer is D                                                                                       1\n",
    "#Brahminic orthodoxy, so the answer is A\n",
    "#So I will predict that we should be able to fix most cases by simply identifying the long results, then just taking the last letter as the result\n",
    "#However this is not what we were asked to do so I will leave it out for now\n",
    "print(df_x_filtered.value_counts())\n",
    "#From here we can see that the output so far seems to be <10 characters, and the only weird results will be the ones in the format 'Answer: B' for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b23f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "question_id  result\n",
      "0            B         1\n",
      "9367         A         1\n",
      "9396         B         1\n",
      "9397         A         1\n",
      "9399         A         1\n",
      "                      ..\n",
      "4638         A         1\n",
      "4639         A         1\n",
      "4640         D         1\n",
      "4641         C         1\n",
      "14041        A         1\n",
      "Length: 13436, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question_id, result]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B\n",
    "#Deep = true means that it creates a new dataframe with a copy of everything we are trying to save\n",
    "#First we get rid of the whitespace, especially for the results with 'Answer: B' format\n",
    "def clean_answer(raw_answer, dash_pattern='-'):\n",
    "    \n",
    "    #Remove any whitespace and convert to string in case of anomalies\n",
    "    ans = str(raw_answer).strip()\n",
    "    \n",
    "    # Remove any \"Answer:\"(now we removed spaces so we don't need to remove any spaces)\n",
    "    ans = ans.replace(\"Answer:\", \"\").strip()\n",
    "    \n",
    "    #if it does have a dash pattern, we can remove it (based on the example above)\n",
    "    ans = ans.replace(dash_pattern, \"\").strip()\n",
    "    \n",
    "    #finally to make sure we are beign case-sensitive, we convert everything to uppercase\n",
    "    ans = ans.upper()\n",
    "    \n",
    "    # Sanity check to see that the answers make sense\n",
    "    valid_answers = {\"A\", \"B\", \"C\", \"D\"}\n",
    "    if ans not in valid_answers:\n",
    "        print('none')\n",
    "        return None  # give a placeholder for now if it doesn't work properly and print 'none' to see if anything has gone wrong\n",
    "    \n",
    "    return ans\n",
    "\n",
    "df_x_filtered[\"result\"] = df_x_filtered[\"result\"].apply(clean_answer)\n",
    "df_y_filtered[\"result\"] = df_y_filtered[\"result\"].apply(clean_answer)\n",
    "df_z_filtered[\"result\"] = df_z_filtered[\"result\"].apply(clean_answer)\n",
    "#We could also choose to drop any values, as  \n",
    "print(df_x_filtered.value_counts())\n",
    "#After looking through the filtered X results, we can see that there don't appear to be any mismatches anymore, and we will remove the None results as well\n",
    "df_x_filtered = df_x_filtered.dropna(subset=[\"result\"])\n",
    "df_y_filtered = df_y_filtered.dropna(subset=[\"result\"])\n",
    "df_z_filtered = df_z_filtered.dropna(subset=[\"result\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_x shape: (13882, 2)\n",
      "Filtered df_x shape: (13436, 2)\n",
      "\n",
      "Original df_y shape: (13978, 2)\n",
      "Filtered df_y shape: (13551, 2)\n",
      "\n",
      "Original df_z shape: (13923, 2)\n",
      "Filtered df_z shape: (12753, 2)\n",
      "Series([], Name: result, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original df_x shape: {df_x.shape}\")\n",
    "print(f\"Filtered df_x shape: {df_x_filtered.shape}\\n\")\n",
    "\n",
    "print(f\"Original df_y shape: {df_y.shape}\")\n",
    "print(f\"Filtered df_y shape: {df_y_filtered.shape}\\n\")\n",
    "\n",
    "print(f\"Original df_z shape: {df_z.shape}\")\n",
    "print(f\"Filtered df_z shape: {df_z_filtered.shape}\")\n",
    "\n",
    "#This was just an extra check to see that all the 'None' placeholders were filtered out as I was getting confused about the data size\n",
    "bad_rows = df_x_filtered[df_x_filtered[\"result\"].isin([\"none\", \"None\"])]\n",
    "print(bad_rows[\"result\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33258d5",
   "metadata": {},
   "source": [
    "C. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729aeace",
   "metadata": {},
   "source": [
    "Looking at the previous and current sizes/shapes of the dataframes, we can see that cleaning did remove a lot of rows from the datasets. It is what we expect, as we remove anything bigger than 10. However, I imagine we could still have a lot more rows if we did less extreme filtering for the length. I have done so below, and we can see that for dataframe X, we ended up with 13570 instead of the filtered 13436, an extra 134 rows, and same for the other dataframes. This is something to consider during the filtering process. The upside of this more flexible approach is that we don‚Äôt lose potentially valuable responses that include extra text or slight variations in formatting. On the downside, it does introduce a small risk of incorporating ‚Äúmessy‚Äù data if that extra text doesn‚Äôt consistently end with a valid letter or if it includes typos, however from the results below I seem to only see answers with the allowed format, so I think it worked successfully, primarily removing the 'None' seems to have been the only operation it performed.\n",
    "For now, I will move forward with the alternate data, because I personally think the data will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_x_alt shape: (13570, 2)\n",
      "df_y_alt shape: (13681, 2)\n",
      "df_z_alt shape: (13334, 2)\n",
      "\n",
      "Value counts for df_x_alt:\n",
      "A    5844\n",
      "B    3003\n",
      "C    2373\n",
      "D    2350\n",
      "Name: result, dtype: int64\n",
      "\n",
      "Value counts for df_y_alt:\n",
      "D    5808\n",
      "C    3274\n",
      "B    2545\n",
      "A    2054\n",
      "Name: result, dtype: int64\n",
      "\n",
      "Value counts for df_z_alt:\n",
      "D    3498\n",
      "C    3406\n",
      "B    3278\n",
      "A    3152\n",
      "Name: result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make copies of the dataframes for this test\n",
    "df_x_alt = df_x.copy()\n",
    "df_y_alt = df_y.copy()\n",
    "df_z_alt = df_z.copy()\n",
    "\n",
    "def clean_answerTest(raw_answer, dash_pattern='-'):\n",
    "    \n",
    "    # Same logic as before\n",
    "    ans = str(raw_answer).strip()\n",
    "    \n",
    "    # Remove any \"Answer:\" text\n",
    "    ans = ans.replace(\"Answer:\", \"\").strip()\n",
    "    ans = ans.replace(dash_pattern, \"\").strip()\n",
    "    ans = ans.upper()\n",
    "    \n",
    "    # Take the last character in the string (this is the extra logic I wanted to test)\n",
    "    if ans:\n",
    "        ans = ans[-1]\n",
    "    else:\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Check if it's one of A, B, C, D (removed the 'none' print because it was getting annoying)\n",
    "    valid_answers = {\"A\", \"B\", \"C\", \"D\"}\n",
    "    if ans not in valid_answers:\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    return ans\n",
    "\n",
    "# Apply this cleaning function to the copies\n",
    "df_x_alt[\"result\"] = df_x_alt[\"result\"].apply(clean_answerTest)\n",
    "df_y_alt[\"result\"] = df_y_alt[\"result\"].apply(clean_answerTest)\n",
    "df_z_alt[\"result\"] = df_z_alt[\"result\"].apply(clean_answerTest)\n",
    "\n",
    "#Drop the rows of 'None' (same as above)\n",
    "df_x_alt = df_x_alt.dropna(subset=[\"result\"])\n",
    "df_y_alt = df_y_alt.dropna(subset=[\"result\"])\n",
    "df_z_alt = df_z_alt.dropna(subset=[\"result\"])\n",
    "\n",
    "# check the only answers are in the right format\n",
    "print(\"df_x_alt shape:\", df_x_alt.shape)\n",
    "print(\"df_y_alt shape:\", df_y_alt.shape)\n",
    "print(\"df_z_alt shape:\", df_z_alt.shape)\n",
    "\n",
    "print(\"\\nValue counts for df_x_alt:\")\n",
    "print(df_x_alt[\"result\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for df_y_alt:\")\n",
    "print(df_y_alt[\"result\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for df_z_alt:\")\n",
    "print(df_z_alt[\"result\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda2920",
   "metadata": {},
   "source": [
    "### 1.4 (3 pt)\n",
    "\n",
    "Now that our answer columns are nicely formatted, let's take a look at model performance:\n",
    "\n",
    "A. Both the `MMLU` dataframes and the language model score data frames have the columns `question_id`. For each of the language model score data frames, use an inner join operation with the `df_test` dataframe on the `question_id` column.\n",
    "\n",
    "B. Add a new column to each of the resulting dataframes called `correct`, that checks if the model's answer in `result` is the same as the expected answer in the column `answer`. Then, print the average score of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cfcb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "# I am moving forward with the alt data from the filtering I used above (the second version, including the longer results)\n",
    "df_x_merged_alt = df_x_alt.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n",
    "df_y_merged_alt = df_y_alt.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n",
    "df_z_merged_alt = df_z_alt.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4814bb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X alt: 0.768\n",
      "Y alt: 0.746\n",
      "Z alt: 0.662\n"
     ]
    }
   ],
   "source": [
    "# B\n",
    "#Now we chekc how many were correct (comparing the results and answers)\n",
    "df_x_merged_alt[\"correct\"] = df_x_merged_alt[\"result\"] == df_x_merged_alt[\"answer\"]\n",
    "df_y_merged_alt[\"correct\"] = df_y_merged_alt[\"result\"] == df_y_merged_alt[\"answer\"]\n",
    "df_z_merged_alt[\"correct\"] = df_z_merged_alt[\"result\"] == df_z_merged_alt[\"answer\"]\n",
    "\n",
    "# Compute average score (the mean of the boolean 'correct')\n",
    "x_score = df_x_merged_alt[\"correct\"].mean()\n",
    "y_score = df_y_merged_alt[\"correct\"].mean()\n",
    "z_score = df_z_merged_alt[\"correct\"].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"X alt: {x_score:.3f}\")\n",
    "print(f\"Y alt: {y_score:.3f}\")\n",
    "print(f\"Z alt: {z_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96efc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0.767\n",
      "Y: 0.746\n",
      "Z: 0.663\n"
     ]
    }
   ],
   "source": [
    "#I will also compute it for the old dataset with the more primitive filtering\n",
    "df_x_merged = df_x_filtered.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n",
    "df_y_merged = df_y_filtered.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n",
    "df_z_merged = df_z_filtered.merge(df_test[[\"question_id\", \"answer\",\"subject\"]], on=\"question_id\", how=\"inner\")\n",
    "df_x_merged[\"correct\"] = df_x_merged[\"result\"] == df_x_merged[\"answer\"]\n",
    "df_y_merged[\"correct\"] = df_y_merged[\"result\"] == df_y_merged[\"answer\"]\n",
    "df_z_merged[\"correct\"] = df_z_merged[\"result\"] == df_z_merged[\"answer\"]\n",
    "\n",
    "# Compute average score (the mean of the boolean 'correct')\n",
    "x_score = df_x_merged[\"correct\"].mean()\n",
    "y_score = df_y_merged[\"correct\"].mean()\n",
    "z_score = df_z_merged[\"correct\"].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"X: {x_score:.3f}\")\n",
    "print(f\"Y: {y_score:.3f}\")\n",
    "print(f\"Z: {z_score:.3f}\")\n",
    "#Here we can see the results are very similar, so it didn't make a huge difference, however the alt results should represent the mean better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d719d",
   "metadata": {},
   "source": [
    "### 1.5 (5 pt)\n",
    "\n",
    "Hmmm, something doesn't seem quite right. Let's investigate how \"balanced\" this dataset is:\n",
    "\n",
    "A. For each of the 57 subjects in the MMLU, compare the number of questions answered by each model. Print the subjects for which there is a more than 10% difference.\n",
    "\n",
    "B. Propose and implement a reasonable way to rebalance the results. (e.g., while throwing away 100% of the results perfectly rebalances the results, it is not reasonable).\n",
    "\n",
    "C. Finally, print the updated accuracy on the rebalanced data.\n",
    "\n",
    "**hint:**:\n",
    "- (A) For a given subject, let model X and model Y have answered 181 and 200 questions respectively. You can consider this a 10% difference from the perspective of X, i.e., (200 - 181) / 181 > 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1634b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question_id', 'result', 'answer', 'subject', 'correct'], dtype='object')\n",
      "Index(['question_id', 'result', 'answer', 'subject', 'correct'], dtype='object')\n",
      "Index(['question_id', 'result', 'answer', 'subject', 'correct'], dtype='object')\n",
      "subject\n",
      "abstract algebra                         95\n",
      "anatomy                                 129\n",
      "astronomy                               144\n",
      "business ethics                          98\n",
      "clinical knowledge                      259\n",
      "college biology                         139\n",
      "college chemistry                        96\n",
      "college computer science                 97\n",
      "college mathematics                      99\n",
      "college medicine                        168\n",
      "college physics                          98\n",
      "computer security                        95\n",
      "conceptual physics                      226\n",
      "econometrics                            112\n",
      "electrical engineering                  141\n",
      "elementary mathematics                  367\n",
      "formal logic                            109\n",
      "global facts                             98\n",
      "high school biology                     297\n",
      "high school chemistry                   196\n",
      "high school computer science             98\n",
      "high school european history            158\n",
      "high school geography                   195\n",
      "high school government and politics     185\n",
      "high school macroeconomics              381\n",
      "high school mathematics                 264\n",
      "high school microeconomics              233\n",
      "high school physics                     149\n",
      "high school psychology                  532\n",
      "high school statistics                  210\n",
      "high school us history                  198\n",
      "high school world history               224\n",
      "human aging                             216\n",
      "human sexuality                         129\n",
      "international law                       114\n",
      "jurisprudence                           102\n",
      "logical fallacies                       154\n",
      "machine learning                        103\n",
      "management                              102\n",
      "marketing                               229\n",
      "medical genetics                         97\n",
      "miscellaneous                           759\n",
      "moral disputes                          329\n",
      "moral scenarios                         737\n",
      "nutrition                               297\n",
      "philosophy                              300\n",
      "prehistory                              308\n",
      "professional accounting                 272\n",
      "professional law                       1490\n",
      "professional medicine                   265\n",
      "professional psychology                 587\n",
      "public relations                        106\n",
      "security studies                        231\n",
      "sociology                               195\n",
      "us foreign policy                        95\n",
      "virology                                162\n",
      "world religions                         167\n",
      "Name: count_x, dtype: int64\n",
      "subject\n",
      "abstract algebra                         97\n",
      "anatomy                                 130\n",
      "astronomy                               148\n",
      "business ethics                          96\n",
      "clinical knowledge                      261\n",
      "college biology                         140\n",
      "college chemistry                        98\n",
      "college computer science                 98\n",
      "college mathematics                     100\n",
      "college medicine                        167\n",
      "college physics                         100\n",
      "computer security                        98\n",
      "conceptual physics                      225\n",
      "econometrics                            107\n",
      "electrical engineering                  141\n",
      "elementary mathematics                  368\n",
      "formal logic                            123\n",
      "global facts                             97\n",
      "high school biology                     296\n",
      "high school chemistry                   198\n",
      "high school computer science             95\n",
      "high school european history            161\n",
      "high school geography                   193\n",
      "high school government and politics     188\n",
      "high school macroeconomics              377\n",
      "high school mathematics                 265\n",
      "high school microeconomics              233\n",
      "high school physics                     145\n",
      "high school psychology                  529\n",
      "high school statistics                  207\n",
      "high school us history                  198\n",
      "high school world history               233\n",
      "human aging                             214\n",
      "human sexuality                         129\n",
      "international law                       119\n",
      "jurisprudence                           103\n",
      "logical fallacies                       136\n",
      "machine learning                        108\n",
      "management                               98\n",
      "marketing                               225\n",
      "medical genetics                         98\n",
      "miscellaneous                           755\n",
      "moral disputes                          304\n",
      "moral scenarios                         865\n",
      "nutrition                               303\n",
      "philosophy                              303\n",
      "prehistory                              317\n",
      "professional accounting                 271\n",
      "professional law                       1488\n",
      "professional medicine                   264\n",
      "professional psychology                 587\n",
      "public relations                        105\n",
      "security studies                        232\n",
      "sociology                               194\n",
      "us foreign policy                        95\n",
      "virology                                158\n",
      "world religions                         168\n",
      "Name: count_y, dtype: int64\n",
      "subject\n",
      "abstract algebra                         95\n",
      "anatomy                                 125\n",
      "astronomy                               137\n",
      "business ethics                          92\n",
      "clinical knowledge                      244\n",
      "college biology                         131\n",
      "college chemistry                        84\n",
      "college computer science                 84\n",
      "college mathematics                      93\n",
      "college medicine                        161\n",
      "college physics                          91\n",
      "computer security                        87\n",
      "conceptual physics                      212\n",
      "econometrics                            103\n",
      "electrical engineering                  138\n",
      "elementary mathematics                  348\n",
      "formal logic                            113\n",
      "global facts                             92\n",
      "high school biology                     277\n",
      "high school chemistry                   185\n",
      "high school computer science             90\n",
      "high school european history            147\n",
      "high school geography                   176\n",
      "high school government and politics     177\n",
      "high school macroeconomics              350\n",
      "high school mathematics                 247\n",
      "high school microeconomics              221\n",
      "high school physics                     136\n",
      "high school psychology                  515\n",
      "high school statistics                  207\n",
      "high school us history                  192\n",
      "high school world history               222\n",
      "human aging                             202\n",
      "human sexuality                         122\n",
      "international law                       114\n",
      "jurisprudence                           100\n",
      "logical fallacies                       147\n",
      "machine learning                        107\n",
      "management                               93\n",
      "marketing                               217\n",
      "medical genetics                         89\n",
      "miscellaneous                           716\n",
      "moral disputes                          250\n",
      "moral scenarios                         774\n",
      "nutrition                               286\n",
      "philosophy                              282\n",
      "prehistory                              299\n",
      "professional accounting                 261\n",
      "professional law                       1390\n",
      "professional medicine                   257\n",
      "professional psychology                 564\n",
      "public relations                         97\n",
      "security studies                        230\n",
      "sociology                               188\n",
      "us foreign policy                        92\n",
      "virology                                150\n",
      "world religions                         154\n",
      "Name: count_z, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#A I will just continue with the original filtered results\n",
    "print(df_x_merged.columns)\n",
    "print(df_y_merged.columns)\n",
    "print(df_z_merged.columns)\n",
    "#went back and added the merge to include subjects (oversight on my part)\n",
    "count_x = df_x_merged.groupby(\"subject\")[\"question_id\"].nunique().rename(\"count_x\")\n",
    "count_y = df_y_merged.groupby(\"subject\")[\"question_id\"].nunique().rename(\"count_y\")\n",
    "count_z = df_z_merged.groupby(\"subject\")[\"question_id\"].nunique().rename(\"count_z\")\n",
    "print(count_x)\n",
    "print(count_y)\n",
    "print(count_z)\n",
    "#Now we have the counts for everything, time to look at it side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcdc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     count_x  count_y  count_z\n",
      "subject                                                       \n",
      "abstract algebra                          95       97       95\n",
      "anatomy                                  129      130      125\n",
      "astronomy                                144      148      137\n",
      "business ethics                           98       96       92\n",
      "clinical knowledge                       259      261      244\n",
      "college biology                          139      140      131\n",
      "college chemistry                         96       98       84\n",
      "college computer science                  97       98       84\n",
      "college mathematics                       99      100       93\n",
      "college medicine                         168      167      161\n",
      "college physics                           98      100       91\n",
      "computer security                         95       98       87\n",
      "conceptual physics                       226      225      212\n",
      "econometrics                             112      107      103\n",
      "electrical engineering                   141      141      138\n",
      "elementary mathematics                   367      368      348\n",
      "formal logic                             109      123      113\n",
      "global facts                              98       97       92\n",
      "high school biology                      297      296      277\n",
      "high school chemistry                    196      198      185\n",
      "high school computer science              98       95       90\n",
      "high school european history             158      161      147\n",
      "high school geography                    195      193      176\n",
      "high school government and politics      185      188      177\n",
      "high school macroeconomics               381      377      350\n",
      "high school mathematics                  264      265      247\n",
      "high school microeconomics               233      233      221\n",
      "high school physics                      149      145      136\n",
      "high school psychology                   532      529      515\n",
      "high school statistics                   210      207      207\n",
      "high school us history                   198      198      192\n",
      "high school world history                224      233      222\n",
      "human aging                              216      214      202\n",
      "human sexuality                          129      129      122\n",
      "international law                        114      119      114\n",
      "jurisprudence                            102      103      100\n",
      "logical fallacies                        154      136      147\n",
      "machine learning                         103      108      107\n",
      "management                               102       98       93\n",
      "marketing                                229      225      217\n",
      "medical genetics                          97       98       89\n",
      "miscellaneous                            759      755      716\n",
      "moral disputes                           329      304      250\n",
      "moral scenarios                          737      865      774\n",
      "nutrition                                297      303      286\n",
      "philosophy                               300      303      282\n",
      "prehistory                               308      317      299\n",
      "professional accounting                  272      271      261\n",
      "professional law                        1490     1488     1390\n",
      "professional medicine                    265      264      257\n",
      "professional psychology                  587      587      564\n",
      "public relations                         106      105       97\n",
      "security studies                         231      232      230\n",
      "sociology                                195      194      188\n",
      "us foreign policy                         95       95       92\n",
      "virology                                 162      158      150\n",
      "world religions                          167      168      154\n",
      "                          count_x  count_y  count_z   diff_xy   diff_xz  \\\n",
      "subject                                                                   \n",
      "college chemistry              96       98       84  0.020833  0.125000   \n",
      "college computer science       97       98       84  0.010309  0.134021   \n",
      "computer security              95       98       87  0.031579  0.084211   \n",
      "formal logic                  109      123      113  0.128440  0.036697   \n",
      "logical fallacies             154      136      147  0.116883  0.045455   \n",
      "moral disputes                329      304      250  0.075988  0.240122   \n",
      "moral scenarios               737      865      774  0.173677  0.050204   \n",
      "\n",
      "                           diff_yz  \n",
      "subject                             \n",
      "college chemistry         0.142857  \n",
      "college computer science  0.142857  \n",
      "computer security         0.112245  \n",
      "formal logic              0.081301  \n",
      "logical fallacies         0.080882  \n",
      "moral disputes            0.177632  \n",
      "moral scenarios           0.105202  \n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "df_counts = pd.concat([count_x, count_y, count_z], axis=1).fillna(0)\n",
    "print(df_counts)\n",
    "#Now we can see the differences, lets calculate!\n",
    "df_counts[\"diff_xy\"] = (df_counts[\"count_y\"] - df_counts[\"count_x\"]).abs() / df_counts[\"count_x\"].clip(lower=1)\n",
    "df_counts[\"diff_xz\"] = (df_counts[\"count_z\"] - df_counts[\"count_x\"]).abs() / df_counts[\"count_x\"].clip(lower=1)\n",
    "df_counts[\"diff_yz\"] = (df_counts[\"count_z\"] - df_counts[\"count_y\"]).abs() / df_counts[\"count_y\"].clip(lower=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a5524fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          count_x  count_y  count_z   diff_xy   diff_xz  \\\n",
      "subject                                                                   \n",
      "college chemistry              96       98       84  0.020833  0.125000   \n",
      "college computer science       97       98       84  0.010309  0.134021   \n",
      "computer security              95       98       87  0.031579  0.084211   \n",
      "formal logic                  109      123      113  0.128440  0.036697   \n",
      "logical fallacies             154      136      147  0.116883  0.045455   \n",
      "moral disputes                329      304      250  0.075988  0.240122   \n",
      "moral scenarios               737      865      774  0.173677  0.050204   \n",
      "\n",
      "                           diff_yz  \n",
      "subject                             \n",
      "college chemistry         0.142857  \n",
      "college computer science  0.142857  \n",
      "computer security         0.112245  \n",
      "formal logic              0.081301  \n",
      "logical fallacies         0.080882  \n",
      "moral disputes            0.177632  \n",
      "moral scenarios           0.105202  \n"
     ]
    }
   ],
   "source": [
    "#Here we check if it is actually imbalanced (based on the rules given above)\n",
    "imbalanced_subjects = df_counts[\n",
    "    (df_counts[\"diff_xy\"] > 0.1) |\n",
    "    (df_counts[\"diff_xz\"] > 0.1) |\n",
    "    (df_counts[\"diff_yz\"] > 0.1)\n",
    "]\n",
    "print(imbalanced_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects with >10% difference:\n",
      "                           count_x  count_y  count_z   diff_xy   diff_xz  \\\n",
      "subject                                                                   \n",
      "college chemistry              96       98       84  0.020833  0.125000   \n",
      "college computer science       97       98       84  0.010309  0.134021   \n",
      "computer security              95       98       87  0.031579  0.084211   \n",
      "formal logic                  109      123      113  0.128440  0.036697   \n",
      "logical fallacies             154      136      147  0.116883  0.045455   \n",
      "moral disputes                329      304      250  0.075988  0.240122   \n",
      "moral scenarios               737      865      774  0.173677  0.050204   \n",
      "\n",
      "                           diff_yz  \n",
      "subject                             \n",
      "college chemistry         0.142857  \n",
      "college computer science  0.142857  \n",
      "computer security         0.112245  \n",
      "formal logic              0.081301  \n",
      "logical fallacies         0.080882  \n",
      "moral disputes            0.177632  \n",
      "moral scenarios           0.105202  \n",
      "Balanced X accuracy: 0.768\n",
      "Balanced Y accuracy: 0.745\n",
      "Balanced Z accuracy: 0.663\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "#The information above looks good, now we need to split it up more\n",
    "df_x_balanced_list = []\n",
    "df_y_balanced_list = []\n",
    "df_z_balanced_list = []\n",
    "#This annoying dataframe loop was done with chatGPT, because I needed to seperate the data for it to make sense properly.\n",
    "for subj, row in df_counts.iterrows():\n",
    "    count_x_sub = int(row[\"count_x\"])\n",
    "    count_y_sub = int(row[\"count_y\"])\n",
    "    count_z_sub = int(row[\"count_z\"])\n",
    "    min_count = min(count_x_sub, count_y_sub, count_z_sub)\n",
    "    if min_count == 0:\n",
    "        continue\n",
    "    x_sub_df = df_x_merged[df_x_merged[\"subject\"] == subj].sample(n=min_count, random_state=0)\n",
    "    y_sub_df = df_y_merged[df_y_merged[\"subject\"] == subj].sample(n=min_count, random_state=0)\n",
    "    z_sub_df = df_z_merged[df_z_merged[\"subject\"] == subj].sample(n=min_count, random_state=0)\n",
    "    df_x_balanced_list.append(x_sub_df)\n",
    "    df_y_balanced_list.append(y_sub_df)\n",
    "    df_z_balanced_list.append(z_sub_df)\n",
    "\n",
    "df_x_balanced = pd.concat(df_x_balanced_list, ignore_index=True)\n",
    "df_y_balanced = pd.concat(df_y_balanced_list, ignore_index=True)\n",
    "df_z_balanced = pd.concat(df_z_balanced_list, ignore_index=True)\n",
    "\n",
    "x_score_balanced = df_x_balanced[\"correct\"].mean()\n",
    "y_score_balanced = df_y_balanced[\"correct\"].mean()\n",
    "z_score_balanced = df_z_balanced[\"correct\"].mean()\n",
    "\n",
    "print(\"Subjects with >10% difference:\\n\", imbalanced_subjects)\n",
    "print(f\"Balanced X accuracy: {x_score_balanced:.3f}\")\n",
    "print(f\"Balanced Y accuracy: {y_score_balanced:.3f}\")\n",
    "print(f\"Balanced Z accuracy: {z_score_balanced:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b40d7",
   "metadata": {},
   "source": [
    "To explain what I just did, first, each model‚Äôs merged DataFrame is grouped by subject to see how many questions it answered. That number is recorded for each subject in three separate counts. These counts are combined into a single table, where any absent subjects are filled with zeros. The code then calculates how much each subject‚Äôs counts differ between models, expressed as a proportion relative to the first model‚Äôs count. If that difference is more than ten percent for any pair of models, that subject is labeled imbalanced. Next, the script rebalances by looping over every subject and finding the smallest count among the three. A sample of that size is drawn for each model, ensuring all three have the same number of questions for the subject. Finally, these sampled rows are concatenated back into one balanced dataset per model (the annoying part), and the new accuracy scores are computed from the ‚Äúcorrect‚Äù column. This approach allows a fairer comparison between models, as each is evaluated on an equal footing in terms of how many questions per subject it sees, as stated above in the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0547c",
   "metadata": {},
   "source": [
    "## Task 2 (26 points): What do you mean A > D > B > C...?\n",
    "\n",
    "Nice work! Having successfully inspected, cleaned, and rebalanced the provided data, you head over to director of the government's FIETS project operating under the code name Geronimo. He is happy with your work so far, but worried that the sloppy intern might have done more undetected damage. To be sure, he orders a new set of evaluations of all models on both MMLU and another dataset.\n",
    "\n",
    "After cleaning up and rebalancing, you are left with the concatenated score files in the second folder `task_2`:\n",
    "```\n",
    "task_2/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_other.csv\n",
    "```\n",
    "\n",
    "Each has a new column called `model_name`, which is one of `X, Y` or `Z`.\n",
    "\n",
    "\n",
    "\n",
    "_NOTE: **only** use data from `task_2` and `task_2_5` for this assignment! The values in `lm_scores_mmlu.csv` will NOT be the same as the dataframes you finished in task 1. This is due to \"randomness\" or \"temperature\" in language model inference. This can slightly shift around generative results. (Conveniently: it also ensures any mistakes made in Task 1 don't propogate further ;) )_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2067ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED CODE\n",
    "df_mmlu = pd.read_csv('data/task_2/lm_scores_mmlu.csv')\n",
    "df_other = pd.read_csv('data/task_2/lm_scores_other.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ed869",
   "metadata": {},
   "source": [
    "### 2.1 (4 pt)\n",
    "\n",
    "Let's explore the new results:\n",
    "\n",
    "A. Compute the mean accuracy and standard errors of each model on both datasets and print the results.\n",
    "\n",
    "B. Then, show your results in a bar plot using standard errors with a 95% confidence interval around the mean. Make sure the plot is easy to read and well annotated.\n",
    "\n",
    "C. /Discuss:/ the plot you created: (i) can you say that one of the models is the best? (ii) is there anything that seems odd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69b5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587c824",
   "metadata": {},
   "source": [
    "### 2.2 (5 pt)\n",
    "\n",
    "Geronimo has assured you that both datasets contain questions of similar difficulty, so, what could be going on here?\n",
    "\n",
    "A. What is the distribution of correct answers (A, B, C, D) for each dataset? Create a bar chart to visualize this.\n",
    "\n",
    "B. Perform a chi-square test at $\\alpha = 0.05$, of independence to determine if there's a significant difference in the distribution of correct answers between the two datasets. What do you conclude?\n",
    "\n",
    "**hints**:\n",
    "- for (A), keep in mind that df_mmlu and df_other contain the results of all models, i.e., the `question_id` column is duplicated.\n",
    "- for (A), take care to clearly annotate the bar chart, e.g., title, y-label, legend.\n",
    "- for (B), clearly state the null hypothesis and alternative hypothesis\n",
    "- use the `chi2_contingency` function from `scipy.stats`\n",
    "- format your results from answer (A) as a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74904507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c461f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f875f",
   "metadata": {},
   "source": [
    "### 2.3 (7 pt)\n",
    "\n",
    "Let's dive in deeper:\n",
    "\n",
    "A. What is language model X's mean accuracy conditioned on the four answer options for each dataset?\n",
    "\n",
    "B. Compare LM X's performance when the correct answer is \"A\" between the two datasets. Use a T-test with CI = 0.95. What do you conclude?\n",
    "\n",
    "C. Compare LM X's performance when the correct answer is \"A\" vs. \"C or D\" for each dataset. Use a T-test with CI = 0.95. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b265e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ecae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d7754",
   "metadata": {},
   "source": [
    "### 2.4 (2 pt)\n",
    "\n",
    "What an intriguing finding! \n",
    "\n",
    "A. Print the mean accuracies conditioned on the correct answer for all LMs for each dataset.\n",
    "\n",
    "B. /Discuss:/ What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77801937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812af06",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31947726",
   "metadata": {},
   "source": [
    "### 2.5 (2 pt)\n",
    "\n",
    "Concerned with your findings so far, you quickly consult with Geronimo. After thinking it over, Geronimo concludes that more tests are needed. He orders a second round of MMLU results. However, Geronimo thinks of the following twist: while keeping questions fixed, he randomly permutes the position of the correct answer. The new results can be found in the folder `data/task_2_5/`:\n",
    "```\n",
    "task_2_5/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu_shuffle.csv\n",
    "```\n",
    "\n",
    "/Discuss:/ Why would Geronimo do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ffc90",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb158adf",
   "metadata": {},
   "source": [
    "### 2.6 (4 pt)\n",
    "\n",
    "Increasingly sceptical of the language models' performance, you read up on proper testing practices. You stumble upon the concept of [test-rested stability](https://en.wikipedia.org/wiki/Repeatability), which roughtly states that:\n",
    "\n",
    "\"_Measurements taken by a single person or instrument on the same item, under the same conditions, and in a short period of time, should have the same results._\"\n",
    "\n",
    "In our case, we would assume an LM would have the same performance on a given question regardless of the correct answer position. One way of testing this is by using the following metric:\n",
    "\n",
    "$$\\text{test-retest metric} = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{M}\\sum_{j=1}^M c^i_0 c_j^i,$$\n",
    "\n",
    "where $c^i_0 \\in \\{0, 1\\}$ indicates whether the model answers the $i^{\\text{th}}$ question correctly (1 if correct, 0 if incorrect). $c_j^i$ indicates whether the model answers the $i^{\\text{th}}$ question correctly in the $j^{\\text{th}}$ shuffled version of the answer label content. Finally, $M$ is the total number of shuffles and $N$ is the dataset size.\n",
    "\n",
    "Task: compute the test-retest metric for each language model using the original `lm_scores_mmlu.csv` file and the new `lm_scores_mmlu_shuffle.csv` file. Using a bar plot, visualize your results by comparing the accuracy of the original `lm_scores_mmlu.csv` and the test-retest scores.\n",
    "\n",
    "**hints**\n",
    "- what is $M$ in our case?\n",
    "\n",
    "(bonus: no points, but so much sweet, sweet knowledge - check out [the following article](https://arxiv.org/pdf/2406.19470v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ce6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fancy code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d52d93",
   "metadata": {},
   "source": [
    "### 2.7 (2 pt)\n",
    "\n",
    "A. Using the unshuffled data: For each LM, print the distribution of the answers they give as well as the accuracy conditioned on the answer they give.\n",
    "\n",
    "B. /Discuss:/ Describe what you observe\n",
    "\n",
    "[bonus: not scored, but again _that sweet, sweet knowledge_] Could you think of a plausible explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579c30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4c2e8",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45a77b",
   "metadata": {},
   "source": [
    "## Task 3 (16 points): What do Questions and Answers look like for a Language Model?\n",
    "\n",
    "While you feel pretty good about the tests you conducted so far, something still bothers you: what if the language models don't see the data like you do? Suddenly, you receive a phone call from a wise AI sage based in Maastricht named Yodata:\n",
    "\n",
    "```\n",
    "\"Hmmm, correct you are, jonge padawan, to question how the wereld is seen by large language models! Simple 'text,' it is not, nee nee nee! Characters and words, the way of gewone humans, this is not, heh heh heh.\n",
    "\n",
    "'Tokens,' they use, ja! Mysterious and powerful, these tokens are. Expand our vocabulary, they do, beyond the simple 'a to Z.' Chunky blocks of text, they become, yes! 'Hello world,' a simple phrase it may seem. But to a language model, '[24912, 2375]' it might appear, hmm? Verwarrend, it is!\n",
    "\n",
    "Wise, it would be, to explore these MMLU data points through the eyes of a language model, you think? Yes, yes! Much to learn, there is. The ways of the tokens, understand you must, if truly comprehend the great LMs, you wish to.\n",
    "\n",
    "Meditate on this, you should. The force of natural language processing, strong it is. But geduld, you must have, my jonge padawan. For only through great study and contemplation, will the mysteries of the tokens reveal themselves to you, they will. Ja, hmmm!\"\n",
    "```\n",
    "\n",
    "Admittingly, Yodata at times speaks in riddles... However, he was explaining a crucial aspect of modern LMs called [Tokenization](https://learn.microsoft.com/en-us/dotnet/ai/conceptual/understanding-tokens):\n",
    "\n",
    "\n",
    "‚ÄúTokens are words, character sets, or combinations of words and punctuation that are used by [language models (LMs)] to decompose text into. Tokenization is the first step in training‚Äù\n",
    "\n",
    "Instead of characters, LMs process natural language using ‚Äútokens‚Äù. While this is useful for a number of reasons, it does at times introduce some ‚Äúunintuitive‚Äù behavior‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "001c4c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-55e4d70d6b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mexample_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hello world'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'humans see: \"{example_string}\" --> language models see: {tokenize_text(example_string)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-55e4d70d6b7a>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt-4o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/model.py\u001b[0m in \u001b[0;36mencoding_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mRaises\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecognised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_name_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/registry.py\u001b[0m in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mconstructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENCODING_CONSTRUCTORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mENCODINGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken_ext/openai_public.py\u001b[0m in \u001b[0;36mo200k_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mo200k_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     mergeable_ranks = load_tiktoken_bpe(\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;34m\"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mexpected_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"446a9538cb6c348e3516120d7c08b09f57c36495e2acfffe59a5bf8b0cfb1a2d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# NB: do not add caching to this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiktoken_bpe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# NB: do not add caching to this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiktoken_bpe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    148\u001b[0m     return {\n\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PROVIDED CODE\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:\n",
    "    print('installing tiktoken package')\n",
    "    \n",
    "    !pip install tiktoken\n",
    "    \n",
    "    import tiktoken\n",
    "\n",
    "def tokenize_text(s):\n",
    "    enc = tiktoken.encoding_for_model('gpt-4o')\n",
    "    tokens = enc.encode(str(s))\n",
    "    return tokens\n",
    "\n",
    "example_string = 'hello world'\n",
    "print(f'humans see: \"{example_string}\" --> language models see: {tokenize_text(example_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab56227",
   "metadata": {},
   "source": [
    "### 3.1 (5 pt)\n",
    "\n",
    "Use the provided code in the cell above to \"see the world through the eyes of a language model\":\n",
    "\n",
    "A. Tokenize the questions of the original MMLU data provided in task 1: `task_1/mmlu_data/test.csv` and plot the token distribution (the frequency of each token).\n",
    "\n",
    "B. Same as (A), but now for the answers in columns (columns \"A\", \"B\", \"C\", and \"D\").\n",
    "\n",
    "C. Isolate the tokens for the strings \"A\", \"B\", \"C\", and \"D\", then, for their occurances in both questions and answers, print their relative distribution to each other.\n",
    "\n",
    "**hint**\n",
    "- There are a _lot_ of tokens, consider using a cutoff point and log scale\n",
    "- For (c), they should sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00f69cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d0055",
   "metadata": {},
   "source": [
    "### 3.2 (3 pt)\n",
    "\n",
    "What if the number of \"A\", \"B\", \"C\", and \"D\" tokens in the question and answer pairs could influence a language model's decisions?\n",
    "\n",
    "A. For each question-answer pair, compute: \n",
    "1. the number of \"A\", \"B\", \"C\", and \"D\" tokens that occur in the combined question and answers; \n",
    "2. an the total number of tokens.\n",
    "3. then, group by the \"correct\" answer and compute the mean frequency of A, B, C, and D tokens and the total number of tokens. \n",
    "4. finally, print your results\n",
    "\n",
    "B. /Discuss:/ What do you think of the hypothesis that the frequency of A, B, C, and D tokens could influence answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf09aaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.243017</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.013035</td>\n",
       "      <td>93.187151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.231947</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.029463</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>88.846332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.226410</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>92.653825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.242850</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>0.030985</td>\n",
       "      <td>0.014301</td>\n",
       "      <td>92.110169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A         B         C         D      total\n",
       "            mean      mean      mean      mean       mean\n",
       "answer                                                   \n",
       "A       0.243017  0.018932  0.025140  0.013035  93.187151\n",
       "B       0.231947  0.019642  0.029463  0.012709  88.846332\n",
       "C       0.226410  0.018984  0.034897  0.015355  92.653825\n",
       "D       0.242850  0.014566  0.030985  0.014301  92.110169"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020511d4",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b42b74",
   "metadata": {},
   "source": [
    "### 3.3 (4 pt)\n",
    "\n",
    "Three of the most important considerations when deciding between language models are:\n",
    "\n",
    "Quality\n",
    "Costs\n",
    "Speed\n",
    "\n",
    "So far, much of your analysis has focused on quality. However, the government has indicated that they are quite concerned about both the total costs and speed as well. Specifically, it has been brought to their attention that a new `turbo` model has been launched! \n",
    "\n",
    "This model is both cheaper and faster than the models you evaluated so far. However, there is a catch: the context length* is much smaller than that of the other LMS. Namely, it can only process **300** tokens during inference. Meanwhile, the other models can process up to 100K tokens! \n",
    "\n",
    "*_The ‚Äúcontext length‚Äù refers to the number of tokens that can be given to an LM as input._\n",
    "\n",
    "A. Are there subjects where using the cheaper model might be problematic? I.e., where part of the question and answer(s) might not fit completely in the context?\n",
    "\n",
    "B. /Discuss:/ Can you think of a strategy that would balance the needs of the government?\n",
    "\n",
    "**hint**:\n",
    "- An LM needs to have both the question and the different answer options in its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f365f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231fdc6",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44558c5d",
   "metadata": {},
   "source": [
    "### 3.4 (4 pt)\n",
    "\n",
    "/Discuss:/ The time has come to give your final recommendation on the use of LMs in education to the government! Taking into account everything you analyzed in all the preceding tasks (1, 2, and 3), please write a short recommendation consisting of 4 bullet points discussing your concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2e634",
   "metadata": {},
   "source": [
    "B. /Discuss:/\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_cours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
